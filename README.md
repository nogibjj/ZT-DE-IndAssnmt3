# Databricks ETL Pipeline Project

## Overview
This project demonstrates an ETL (Extract, Transform, Load) pipeline using Databricks, focusing on HR data analysis. The process includes data ingestion, transformation, storage, and visualization.

## Pipeline Steps
- **Data Ingestion**: Data from a CSV file is ingested into a DataFrame.
- **Data Transformation**: Performed using Spark SQL to generate insights.
- **Data Storage**: Data stored in Delta Lake to leverage versioning and efficient storage.
- **Data Visualization**: Graphical representation of the transformed data for better understanding.

## Conclusion and Recommendation
- The project reveals key insights into the HR dataset, highlighting departmental employee distribution and salary ranges.
- Recommendations include considering salary restructuring to ensure competitive compensation.

## Screenshots
Include the following screenshots in your README:
1. **Data Ingestion Step**: Screenshot of the notebook section where the CSV file is being read.
2. **Data Transformation**: Capture of the Spark SQL queries in action.
3. **Visualizations**: Images of the histograms and bar charts created.
4. **Pipeline Configuration**: Screenshot of the Delta Live Tables setup in Databricks.

Remember to update the paths and specifics to match your project's details.
